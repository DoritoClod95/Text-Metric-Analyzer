{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/Zonx4qFiGzcD1nxASzVk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DoritoClod95/Text-Metric-Analyzer/blob/main/Differences_of_Social_Media_Comments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Title: Difference of Social Media Comment Sections\n",
        "Name: Carla Parinas\n",
        "Student ID: 300653631"
      ],
      "metadata": {
        "id": "f12_Q9-lzaHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. INTRODUCTION AND RESEARCH QUESTION(S)\n",
        "\n",
        "For this project, I want to analyse the differences in behaviour and mannerisms across social media platforms in terms of comments. I saw this video of a dog and noticed how despite being the same video, the comments were drastically different depending on the platorm. As a person who's relatively active on social media apps, things such as social etiquette seems to be present in the online world as well. I tend to notice a difference in comments depending on the social media app, and that although there are no rules to commenting, there seems to be a standard for how comments should be. For example, in the internet, there seems to be a trend of instagram having more vulgar and negative comments compared to tiktok.If the internet is described like a digital world, then the drastic change in comment sections styles could be comparable to something like culture shock, and I found that very intruiging. My research questions for this project would be:\n",
        "\n",
        "- What is the difference in emotion under both social media?\n",
        "- Which comment section is more sophistication in terms of their wording?\n",
        "\n",
        "To answer my research questions, I will be doing calculating things such as:\n",
        "\n",
        "- Lexical Diversity, to see how well the comments varies (although there could be some issues with this)\n",
        "- Most common words and ngrams, to find any trends in wording\n",
        "- Sentiment Value (Using VADER), to see the differences in emotion\n",
        "- Readability formulas, to measure the sophistication and see if it matches the video (maybe try different ones?)\n",
        "- Age of Acquisition, to have rough estimate about the age demographic of a group\n",
        "- Spacy, noun dependencies\n",
        "- Basic word information, in case I need it *\n",
        "\n",
        "### DATA AND DESCRIPTION OF DATA\n",
        "\n",
        "I have decided to have 2 categories:\n",
        "\n",
        "1. Tiktok\n",
        "2. Instagram\n",
        "\n",
        "Due to comments having a naturally shorter word count, I will be gathering around 7 videos that are available on both the platforms and putting comments until it reaaches around 250-500 words for each video. If I find that there is an insufficient count for one of the platforms then I will make an exception and add a bit more, but I will include take note of this in the program. The number of comments will be used as data.\n",
        "\n",
        "The video must be available on all the three platforms. I will use videos that aim to have the most insightful comment sections, and I will also use videos that are relatively mainstream -- this is to avoid incredibly niche topics whilest still staying in the area of the category. The comments would have to be different, meaning that the trend of a comment section \"repeating itself\" for humour will not count. I will try to keep these videos neutral as possible. I will not be including emojis or any sort of username tagging.   \n",
        "\n",
        "### Program explanation\n",
        "\n",
        "My program is mostly recycled code from my previous assignment, and some taken from the course. It uses the NLTK library and the spaCy library as the main ones for the linguistics statistics. Before doing the text analysis I made a corpus that I would work with, and then categorized it to either Instagram (ig) or Tiktok (tt). I would feed this corpus into a main function that would run other functions to get the metrics. The only preprocessing I did was removing the punctuation. I didn't remove stopwords because I feel like they are a key when it comes to comments, and social media comments are already short enough as it is. I retained the full stops so that I was able to process the sentences.\n",
        "\n",
        "Note that most, if not, all of the metrics was gathered into an average to keep fairness. The first metric function it would run is text info, which would pretty much gather the basic information of a text: word count, sentence count, comment count, and syllable count. The syllables were counted using the cmu dictionary from the nltk library, otherwise they were done manually. These pieces of information would then be sent to the other functions to get the other metrics. The sentiment score, which was the next metric, was calculated using the vader tool from the nltk library.The next metric was the flesch-reading ease calculator, and I took this from the course, but it takes the values of the text and uses a helper method to calculate the readability. Then it calculated the Age of Acquisition using the help of NLTK library. With the help of spaCy andn the code from the course notebooks, I got the noun dependency of the overall comment section and also the average of each individual comment. Lastly, the program gets the lexical diversity. It gathers the Lexical Diversity of the entire thing, then the average of all the sentences, and then the average of all the comments.\n",
        "\n",
        "I analyzed each of the comparison results individually and then manually counted the amount of times a category was \"higher\" than the other in a specific aspect.\n"
      ],
      "metadata": {
        "id": "TgAdyGUKQ7Vb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1V041cKQmx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e462d7-c61a-4508-d91b-1603e274b5f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n",
            "[nltk_data]  Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    Package stopwords is already up-to-date!\n",
            "[nltk_data]  Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]      /root/nltk_data...\n",
            "[nltk_data]    Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]        date!\n",
            "[nltk_data]  Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]  Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# SETUP\n",
        "import nltk\n",
        "import requests\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import cmudict\n",
        "from nltk import FreqDist\n",
        "from google.colab import drive\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from collections import defaultdict\n",
        "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
        "from nltk.util import ngrams\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "corpus_location = '/content/drive/MyDrive/comments'\n",
        "\n",
        "resources =  ['book', 'stopwords', 'averaged_perceptron_tagger', 'vader_lexicon', 'punkt']\n",
        "nltk.download(resources)\n",
        "\n",
        "updated_stopwords = stopwords.words('english')\n",
        "updated_stopwords.append(\"i'm\")\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "cmu = cmudict.dict()\n",
        "\n",
        "noun_tags = ['NN', 'NNS']\n",
        "verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTIONS\n",
        "\n",
        "def preprocess(text, stopwords):\n",
        "  # print(text)\n",
        "  text = text.lower()\n",
        "  new_text = cleanPunc(text)\n",
        "  if stopwords == False:\n",
        "    new_text = remove_stopwords(text)\n",
        "\n",
        "  text_fdist = FreqDist(new_text)\n",
        "  #print(text_fdist)\n",
        "\n",
        "  return new_text\n",
        "\n",
        "def cleanPunc(text):\n",
        "  punct = ',;\"!\\'[]{}:><-_?``()#$'\n",
        "  #print(text)\n",
        "  text = ''.join([x for x in text if x not in punct and x != ''])\n",
        "\n",
        "  return text\n",
        "\n",
        "def removeFullStop(text):\n",
        "  return ' '.join([x for x in text.split() if x != \".\"])\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    text = ' '.join([x for x in text.split() if x.lower() not in updated_stopwords])\n",
        "\n",
        "    # needed to do it one more time\n",
        "    text = ' '.join([x for x in nltk.word_tokenize(text) if x.lower() not in updated_stopwords])\n"
      ],
      "metadata": {
        "id": "5u8HJCFcARyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_rating_resource(url):\n",
        "  \"\"\"helper function to get lexical resources for LING226 students\n",
        "  resources are hosted on github as .txt in the form of Word\\tValue\\n\n",
        "  \"\"\"\n",
        "  # read the raw text and split on newlines\n",
        "  raw = requests.get(url).text.split('\\n')\n",
        "\n",
        "  # split each pair and convert value to rounded float\n",
        "  # the if statement is there to avoid indexing errors when a row in a resource doesn't have complete data\n",
        "  raw_list = [(pair.split('\\t')[0], round(float(pair.split('\\t')[1]), 3)) for pair in raw if len(pair.split('\\t')) == 2]\n",
        "\n",
        "  # create a dictionary and return it\n",
        "  return dict(raw_list)\n",
        "\n",
        "aoa_url = 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/lexical-resources/AoA_Brysbart.txt'\n",
        "aoa_dict = get_word_rating_resource(aoa_url)"
      ],
      "metadata": {
        "id": "XaZdqo_DVipj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_LD(tokens):\n",
        "  if len(tokens) != 0:\n",
        "    return len(set(tokens))/len(tokens)\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "8vBE5XFAAbUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def syllable_counter(word):\n",
        "  syllables = []\n",
        "  if word in cmu.keys():\n",
        "    #print(\"scenario 1\")\n",
        "    phones = cmu[word][0]\n",
        "    #print(phones)\n",
        "\n",
        "    vowel_sounds = [sound for sound in phones if sound[-1].isdigit()]\n",
        "    #print(vowel_sounds)\n",
        "    syllables = len(vowel_sounds)\n",
        "\n",
        "  else:\n",
        "    #print(\"scenario 2\")\n",
        "    vowels = \"aeiouy\"\n",
        "    syllables = 0\n",
        "    prev_char_is_vowel = False\n",
        "    for char in word:\n",
        "        if char in vowels:\n",
        "            if not prev_char_is_vowel:\n",
        "                syllables += 1\n",
        "            prev_char_is_vowel = True\n",
        "        else:\n",
        "             prev_char_is_vowel = False\n",
        "\n",
        "    #print(syllables)\n",
        "  return syllables"
      ],
      "metadata": {
        "id": "no5zErJKAfQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentimentFinder2(corpus, title):\n",
        "  output = []\n",
        "\n",
        "  raw_file = corpus.raw(title)\n",
        "  text = raw_file.replace('\\r\\n', '.')\n",
        "  sentences = [sent for sent in text.split(\".\") if sent != '' or sent != ' ']\n",
        "\n",
        "  whole_comment = sid.polarity_scores(text)['compound']\n",
        "\n",
        "  for sent in sentences:\n",
        "      output.append(sid.polarity_scores(sent)['compound'])\n",
        "\n",
        "\n",
        "  if output:\n",
        "      sent_avg = sum(output)/len(output)\n",
        "      print(f\"Total Polarity Score: { whole_comment } \\nAverage Polarity Score per Sentence: { sent_avg }\")"
      ],
      "metadata": {
        "id": "mQUofN-1gASJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_info(text):\n",
        "  comments_whole = '\\n'.join(text)\n",
        "  sents_whole = '. '.join(text)\n",
        "  #print(sents_whole)\n",
        "\n",
        "  #print(type(sents_whole))\n",
        "\n",
        "  # lowercase the text\n",
        "  comment = comments_whole.lower()\n",
        "  sents = sents_whole.lower()\n",
        "\n",
        "  # get number of comments\n",
        "  comments = [word for word in comments_whole.split('\\n') if word != '']\n",
        "\n",
        "  #print('comment done')\n",
        "\n",
        "  # extract tokens, removing any that are just punctuation\n",
        "  tokens = [token.lower() for token in nltk.word_tokenize(comment) if token.isalpha()]\n",
        "\n",
        "  #print('token done')\n",
        "\n",
        "  # extract sentences\n",
        "  sentences = [word for word in sents_whole.split('.') if word != '' or word != ' ']\n",
        "  #print(sentences)\n",
        "\n",
        "  #print('sents done')\n",
        "\n",
        "  # extract syllables\n",
        "  syllables = 0\n",
        "\n",
        "\n",
        "  for token in tokens:\n",
        "    syllables += syllable_counter(token)\n",
        "\n",
        "  print('syllable done')\n",
        "  return tokens, sentences, syllables, comments"
      ],
      "metadata": {
        "id": "l7fnYRvVAiK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentimentFinder(sentences, comments_whole):\n",
        "  output = []\n",
        "\n",
        "  print(type(sentences))\n",
        "  #raw_file = corpus.raw(title)\n",
        "  #text = raw_file.replace('\\r\\n', '.')\n",
        "  #sentences = [sent for sent in text.split(\".\") if sent != '' or sent != ' ']\n",
        "  sentence_joined = '. '.join(sentences)\n",
        "\n",
        "  whole_comment = sid.polarity_scores(sentence_joined)['compound']\n",
        "\n",
        "  for sent in sentences:\n",
        "      output.append(sid.polarity_scores(sent)['compound'])\n",
        "\n",
        "  if output:\n",
        "      sent_avg = sum(output)/len(output)\n",
        "      print(f\"Total Polarity Score: { round(whole_comment, 4) } \\nAverage Polarity Score per Sentence: { round(sent_avg, 4) }\")"
      ],
      "metadata": {
        "id": "K2UY8egYAqDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flesch_reading_ease(words, sents, sylls, comments):\n",
        "  output = []\n",
        "  overall = calculate(len(words), len(sents), sylls)\n",
        "\n",
        "  for comment in comments:\n",
        "    syll_c = 0\n",
        "    for word in comment.split():\n",
        "      syll_c += syllable_counter(word)\n",
        "\n",
        "    if syll_c > 0 and len(comment.split()) > 0:\n",
        "      output.append(calculate(len(comment.split()), len(comment.split('.')), syll_c))\n",
        "\n",
        "  avg = sum(output)/len(output)\n",
        "  print(f'Overall Score: {round( overall, 3)}, Average Score per Comment: { round(avg, 3)}')"
      ],
      "metadata": {
        "id": "tDVwOFQcAntB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate(words, sents, sylls):\n",
        "\n",
        "  word_sents = words/sents\n",
        "  syll_words = sylls/words\n",
        "\n",
        "  reading_ease_score = 206.835 - (1.015 * word_sents) - (84.6 * syll_words)\n",
        "\n",
        "  #print(f'Flesch Reading Ease Score: {reading_ease_score}')\n",
        "  return reading_ease_score"
      ],
      "metadata": {
        "id": "LCBldBeTAlf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aoa(words, comments):\n",
        "  comment_output = []\n",
        "  whole_avg = get_aoa(words)\n",
        "\n",
        "  for comment in comments:\n",
        "    if get_aoa(comment.split()) is not None:\n",
        "      #print(comment)\n",
        "      comment_output.append(get_aoa(comment.split()))\n",
        "\n",
        "  #print(comment_output)\n",
        "  comment_avg = sum(comment_output)/len(comment_output)\n",
        "  print(f'Whole Average Comment Section AOA: { round(whole_avg, 2)} \\nAverage AOA per Comment: { round(comment_avg, 2) }')\n",
        "\n",
        "  return round(whole_avg, 1), round(comment_avg, 1)\n"
      ],
      "metadata": {
        "id": "4XoEP-OvU3UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_aoa(words):\n",
        "  #print(words)\n",
        "  output = []\n",
        "  for w in words:\n",
        "      if w in aoa_dict.keys():\n",
        "        #print(w)\n",
        "        output.append(aoa_dict[w])\n",
        "\n",
        "  if len(output) != 0:\n",
        "    avg = sum(output)/len(output)\n",
        "    return round(avg, 1)"
      ],
      "metadata": {
        "id": "SssKFAUqW0DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# an updated version of our function\n",
        "import statistics\n",
        "\n",
        "def avg_sd_noun_deps(text_input):\n",
        "  \"\"\"return average and sd of of dependents per head noun in a text\"\"\"\n",
        "  # create spacy doc\n",
        "  tokens = nlp(text_input)\n",
        "\n",
        "  # list to store noun children\n",
        "  n_deps = []\n",
        "\n",
        "  for token in tokens:\n",
        "    # use simple pos tag to find the nouns\n",
        "    if token.pos_ == \"NOUN\":\n",
        "      n_childs = [c for c in token.children]\n",
        "      n_deps.append(len(n_childs))\n",
        "\n",
        "  # safety first\n",
        "  if n_deps:\n",
        "    # in case you want to check what's happening with the numbers\n",
        "    # print(n_deps)\n",
        "    avg = statistics.mean(n_deps)\n",
        "    sd = statistics.stdev(n_deps)\n",
        "    return avg, sd # first number is the average, second is the standard deviation\n",
        "  else:\n",
        "    print('Sorry, no nouns found')"
      ],
      "metadata": {
        "id": "m68F1EDGo-iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_metrics_individual(corpus, title, tri):\n",
        "  raw = corpus.raw(title)\n",
        "  raw = raw.replace('\\r\\n', '')\n",
        "  raw_text = preprocess(raw, True)\n",
        "  raw_nostop = preprocess(raw, False)\n",
        "\n",
        "  comments_list = [comment for comment in raw_text.split('/t')]\n",
        "  comments_whole = '\\n'.join(comments_list)\n",
        "  #print(comments_list)\n",
        "  #print(comments_whole)\n",
        "\n",
        "  print(\"\\n=======TEXT INFO========\")\n",
        "  #print(type(comments_list))\n",
        "  words, sentences, syllables, comments = text_info(comments_list)\n",
        "  print(f'this text has {len(words)} words, {len(sentences)} sentences, and {syllables} syllables, and {len(comments)} comments.')\n",
        "\n",
        "  print(\"\\n=======SENTIMENT FINDER========\")\n",
        "  sentiment = sentimentFinder(sentences, raw_text)\n",
        "\n",
        "  print(\"\\n=======FLESCH READING EASE========\")\n",
        "  overall = flesch_reading_ease(words, sentences, syllables, comments)\n",
        "\n",
        "  print(\"\\n=======AGE OF ACQUISITION========\")\n",
        "  aoa_avg, aoa_comment_avg = aoa(words, comments)\n",
        "\n",
        "  print(\"\\n=======NOUN DEPENDENCIES========\")\n",
        "  whole_dep, whole_dep_sd = avg_sd_noun_deps(comments_whole)\n",
        "\n",
        "  print(f'Average Noun Dependancy: {round(whole_dep, 4)} \\nStandard Deviation: { round(whole_dep_sd, 4)}')\n",
        "\n",
        "  print(\"\\n=======FIND LD========\")\n",
        "  overall_ld = find_LD(comments_whole)\n",
        "  sent_ld = []\n",
        "  comments_ld = []\n",
        "\n",
        "  for sent in sentences:\n",
        "    if sent != '':\n",
        "      add = find_LD(nltk.word_tokenize(sent))\n",
        "      sent_ld.append(add)\n",
        "\n",
        "  for comment in comments:\n",
        "     comments_ld.append(find_LD(nltk.word_tokenize(comment)))\n",
        "\n",
        "  sent_avg = sum(sent_ld)/len(sent_ld)\n",
        "  comment_avg = sum(comments_ld)/len(comments_ld)\n",
        "  print(f'Overall LD: {round(overall_ld, 4)} \\nSentence Average LD: {round(sent_avg, 4)} \\nComment Average LD: {round(comment_avg, 4)}')\n"
      ],
      "metadata": {
        "id": "VoEYEwuzEG_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments_corpus = CategorizedPlaintextCorpusReader(root = corpus_location, fileids = '.*', cat_pattern = '.*(..).txt')\n",
        "\n",
        "type(comments_corpus.raw('dog_ig.txt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0uKof9RA7wD",
        "outputId": "dd1873c7-5692-4c39-d195-2e8e5e0af2dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DOG"
      ],
      "metadata": {
        "id": "qCYiBdw03R7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_metrics_individual(comments_corpus, 'dog_ig.txt', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vToT1qpQuz6m",
        "outputId": "a543fdb7-5e6f-431e-a7a8-d67bb1c0e498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======TEXT INFO========\n",
            "syllable done\n",
            "this text has 240 words, 30 sentences, and 322 syllables, and 27 comments.\n",
            "\n",
            "=======SENTIMENT FINDER========\n",
            "<class 'list'>\n",
            "Total Polarity Score: 0.993 \n",
            "Average Polarity Score per Sentence: 0.1776\n",
            "\n",
            "=======FLESCH READING EASE========\n",
            "Overall Score: 85.21, Average Score per Comment: 81.997\n",
            "\n",
            "=======AGE OF ACQUISITION========\n",
            "Whole Average Comment Section AOA: 5.0 \n",
            "Average AOA per Comment: 5.01\n",
            "\n",
            "=======NOUN DEPENDENCIES========\n",
            "Average Noun Dependancy: 1.2195 \n",
            "Standard Deviation: 0.9357\n",
            "\n",
            "=======FIND LD========\n",
            "Overall LD: 0.02 \n",
            "Sentence Average LD: 0.9211 \n",
            "Comment Average LD: 0.9864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_metrics_individual(comments_corpus, 'dog_tt.txt', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GzCwsu2urhe",
        "outputId": "2277a3c7-278e-45be-ee01-c9683a82ff92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======TEXT INFO========\n",
            "syllable done\n",
            "this text has 219 words, 41 sentences, and 276 syllables, and 36 comments.\n",
            "\n",
            "=======SENTIMENT FINDER========\n",
            "<class 'list'>\n",
            "Total Polarity Score: 0.9617 \n",
            "Average Polarity Score per Sentence: 0.139\n",
            "\n",
            "=======FLESCH READING EASE========\n",
            "Overall Score: 94.794, Average Score per Comment: 92.943\n",
            "\n",
            "=======AGE OF ACQUISITION========\n",
            "Whole Average Comment Section AOA: 4.7 \n",
            "Average AOA per Comment: 4.69\n",
            "\n",
            "=======NOUN DEPENDENCIES========\n",
            "Average Noun Dependancy: 1.9211 \n",
            "Standard Deviation: 1.851\n",
            "\n",
            "=======FIND LD========\n",
            "Overall LD: 0.0249 \n",
            "Sentence Average LD: 0.9475 \n",
            "Comment Average LD: 0.9903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the Dog video, IG had more comments despite it having less words.\n",
        "It is surprisingly the instagram comment section that has a higher sentiment score both in overall and per sentence. Meaning that the IG comments seemingly have a more positive vibe. When it comes to the readability, it seems that the tiktok comments have a higher score by around 10.\n",
        "The AoA is higher on the instagram comments with age 5, but with instagram it is around age 4 but still close to 5. Coming to noun dependencies, the tiktok comments have a higher noun dependency -- this implies that the TT comment section have more variation and have a more detailed way of commenting while the instagram comments are more consistent in comments in a straightforward manner. To accomodate this result, the lexical density of tiktok is only slightly higher than instagram's in all aspects."
      ],
      "metadata": {
        "id": "V78qhOBB_dZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BATHROOM"
      ],
      "metadata": {
        "id": "g7N4ElEm3VzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_metrics_individual(comments_corpus, 'bathroom_ig.txt', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzxDeyyCvE3y",
        "outputId": "13a7abd4-e660-4211-936a-3f996e27a7f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======TEXT INFO========\n",
            "syllable done\n",
            "this text has 374 words, 35 sentences, and 480 syllables, and 21 comments.\n",
            "\n",
            "=======SENTIMENT FINDER========\n",
            "<class 'list'>\n",
            "Total Polarity Score: -0.9208 \n",
            "Average Polarity Score per Sentence: -0.0814\n",
            "\n",
            "=======FLESCH READING EASE========\n",
            "Overall Score: 87.411, Average Score per Comment: 79.854\n",
            "\n",
            "=======AGE OF ACQUISITION========\n",
            "Whole Average Comment Section AOA: 5.0 \n",
            "Average AOA per Comment: 5.07\n",
            "\n",
            "=======NOUN DEPENDENCIES========\n",
            "Average Noun Dependancy: 1.48 \n",
            "Standard Deviation: 1.3692\n",
            "\n",
            "=======FIND LD========\n",
            "Overall LD: 0.0153 \n",
            "Sentence Average LD: 0.8335 \n",
            "Comment Average LD: 0.9148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_metrics_individual(comments_corpus, 'bathroom_tt.txt', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoceZ0IhveEf",
        "outputId": "114160bd-f0ca-4435-a7bc-dd160150f41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======TEXT INFO========\n",
            "syllable done\n",
            "this text has 380 words, 28 sentences, and 504 syllables, and 22 comments.\n",
            "\n",
            "=======SENTIMENT FINDER========\n",
            "<class 'list'>\n",
            "Total Polarity Score: -0.0212 \n",
            "Average Polarity Score per Sentence: -0.1015\n",
            "\n",
            "=======FLESCH READING EASE========\n",
            "Overall Score: 80.854, Average Score per Comment: 78.273\n",
            "\n",
            "=======AGE OF ACQUISITION========\n",
            "Whole Average Comment Section AOA: 4.8 \n",
            "Average AOA per Comment: 4.8\n",
            "\n",
            "=======NOUN DEPENDENCIES========\n",
            "Average Noun Dependancy: 1.2239 \n",
            "Standard Deviation: 0.9345\n",
            "\n",
            "=======FIND LD========\n",
            "Overall LD: 0.0163 \n",
            "Sentence Average LD: 0.9249 \n",
            "Comment Average LD: 0.9431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the transgender bathroom video, both comment sections have a similar number of comments and words. Both videos have negative sentiment. The overall sentiment is much lower with the IG comments, while the average sentence score is lower on the TT comments. This time, the IG comments have a higher readability rate with a decent difference for overall but only a slight difference for the average per comment. The age of acquisition for IG comments remains consistent at age 5 and TT comments being somewhat lower. The noun dependency and standard deviation for IG comments is also higher than TT comments. The general lexical diversity of the TT comments is higher than the IG comments although there is not much of a difference when it comes to overall LD and comment average LD."
      ],
      "metadata": {
        "id": "FqTiQeo1DMvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DRUMMER"
      ],
      "metadata": {
        "id": "lnfYKhKI3Zze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_metrics_individual(comments_corpus, 'drummer_ig.txt', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkJ8aMB_vpkG",
        "outputId": "e2310bf7-32d5-4056-eaca-d7de3c1bae77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======TEXT INFO========\n",
            "syllable done\n",
            "this text has 359 words, 34 sentences, and 485 syllables, and 25 comments.\n",
            "\n",
            "=======SENTIMENT FINDER========\n",
            "<class 'list'>\n",
            "Total Polarity Score: 0.1485 \n",
            "Average Polarity Score per Sentence: -0.0355\n",
            "\n",
            "=======FLESCH READING EASE========\n",
            "Overall Score: 81.825, Average Score per Comment: 88.166\n",
            "\n",
            "=======AGE OF ACQUISITION========\n",
            "Whole Average Comment Section AOA: 5.2 \n",
            "Average AOA per Comment: 4.97\n",
            "\n",
            "=======NOUN DEPENDENCIES========\n",
            "Average Noun Dependancy: 1.4267 \n",
            "Standard Deviation: 1.3772\n",
            "\n",
            "=======FIND LD========\n",
            "Overall LD: 0.016 \n",
            "Sentence Average LD: 0.8533 \n",
            "Comment Average LD: 0.9548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_metrics_individual(comments_corpus, 'drummer_tt.txt', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wu5-XP74vqHR",
        "outputId": "42434c9c-db22-4562-c078-19478c3ee5f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======TEXT INFO========\n",
            "syllable done\n",
            "this text has 323 words, 52 sentences, and 436 syllables, and 34 comments.\n",
            "\n",
            "=======SENTIMENT FINDER========\n",
            "<class 'list'>\n",
            "Total Polarity Score: 0.995 \n",
            "Average Polarity Score per Sentence: 0.0936\n",
            "\n",
            "=======FLESCH READING EASE========\n",
            "Overall Score: 86.333, Average Score per Comment: 80.032\n",
            "\n",
            "=======AGE OF ACQUISITION========\n",
            "Whole Average Comment Section AOA: 4.9 \n",
            "Average AOA per Comment: 4.97\n",
            "\n",
            "=======NOUN DEPENDENCIES========\n",
            "Average Noun Dependancy: 1.3676 \n",
            "Standard Deviation: 0.9911\n",
            "\n",
            "=======FIND LD========\n",
            "Overall LD: 0.0168 \n",
            "Sentence Average LD: 0.8584 \n",
            "Comment Average LD: 0.9345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the contemporary drummer performance art video, we see quite a big difference in sentiment values. The IG comments have a more negative sentiment than TT comments. The overall reading score is higher for the IG comments but the average reading score per sentence is higher for TT comments. The AoA for IG remains on 5 while TT seems to have switched up to around age 5. IG comments have a higher average noun dependency and standard deviation, with a big difference in the latter metric. The lexical diversity of both platforms are almost identical to each other, with the only slightly notable difference being in the average LD per comment."
      ],
      "metadata": {
        "id": "lePmgcGxkh8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HAIRCUT"
      ],
      "metadata": {
        "id": "0wuatbUA3glw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_metrics_individual(comments_corpus, 'haircut_ig.txt', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw_BnFeDwDL9",
        "outputId": "cd12363f-47e6-4dcd-abcc-ae5578de779c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======TEXT INFO========\n",
            "syllable done\n",
            "this text has 262 words, 38 sentences, and 332 syllables, and 23 comments.\n",
            "\n",
            "=======SENTIMENT FINDER========\n",
            "<class 'list'>\n",
            "Total Polarity Score: -0.9404 \n",
            "Average Polarity Score per Sentence: -0.0477\n",
            "\n",
            "=======FLESCH READING EASE========\n",
            "Overall Score: 92.634, Average Score per Comment: 89.265\n",
            "\n",
            "=======AGE OF ACQUISITION========\n",
            "Whole Average Comment Section AOA: 5.0 \n",
            "Average AOA per Comment: 5.0\n",
            "\n",
            "=======NOUN DEPENDENCIES========\n",
            "Average Noun Dependancy: 1.2203 \n",
            "Standard Deviation: 1.0516\n",
            "\n",
            "=======FIND LD========\n",
            "Overall LD: 0.0198 \n",
            "Sentence Average LD: 0.9241 \n",
            "Comment Average LD: 0.9637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_metrics_individual(comments_corpus, 'haircut_tt.txt', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUoLURIYwDgU",
        "outputId": "7851861f-cdb9-43ba-800d-6a0960538172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======TEXT INFO========\n",
            "syllable done\n",
            "this text has 254 words, 40 sentences, and 301 syllables, and 28 comments.\n",
            "\n",
            "=======SENTIMENT FINDER========\n",
            "<class 'list'>\n",
            "Total Polarity Score: 0.9984 \n",
            "Average Polarity Score per Sentence: 0.184\n",
            "\n",
            "=======FLESCH READING EASE========\n",
            "Overall Score: 100.135, Average Score per Comment: 96.462\n",
            "\n",
            "=======AGE OF ACQUISITION========\n",
            "Whole Average Comment Section AOA: 4.8 \n",
            "Average AOA per Comment: 4.96\n",
            "\n",
            "=======NOUN DEPENDENCIES========\n",
            "Average Noun Dependancy: 1.6818 \n",
            "Standard Deviation: 1.3078\n",
            "\n",
            "=======FIND LD========\n",
            "Overall LD: 0.0222 \n",
            "Sentence Average LD: 0.9048 \n",
            "Comment Average LD: 0.9871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The comment count is higher on TT despite it having lesser words than IG. There is a big difference in sentiment as the score for the IG comments have a generally negative sentiment while the TT comments have a generally postiive sentiment. There is a notable difference in readability as the the TT comments is higher than the IG comments. The AoA is quite similar but IG comments still take on a slightly higher age as usual. The TT comments have a higher noun dependency and a higher standard deviation this time, this also applies to the lexical diversity except for the LD for sentence average."
      ],
      "metadata": {
        "id": "zKTomuyEmqfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HOUSEBOAT"
      ],
      "metadata": {
        "id": "8kf0qsZw3kqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_metrics_individual(comments_corpus, 'houseboat_ig.txt', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFQkv12NwKB9",
        "outputId": "f4c5b321-2721-4a59-b784-e6230b055351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======TEXT INFO========\n",
            "syllable done\n",
            "this text has 266 words, 32 sentences, and 364 syllables, and 16 comments.\n",
            "\n",
            "=======SENTIMENT FINDER========\n",
            "<class 'list'>\n",
            "Total Polarity Score: 0.9808 \n",
            "Average Polarity Score per Sentence: 0.1267\n",
            "\n",
            "=======FLESCH READING EASE========\n",
            "Overall Score: 82.629, Average Score per Comment: 84.618\n",
            "\n",
            "=======AGE OF ACQUISITION========\n",
            "Whole Average Comment Section AOA: 4.8 \n",
            "Average AOA per Comment: 4.66\n",
            "\n",
            "=======NOUN DEPENDENCIES========\n",
            "Average Noun Dependancy: 1.1833 \n",
            "Standard Deviation: 1.0167\n",
            "\n",
            "=======FIND LD========\n",
            "Overall LD: 0.0223 \n",
            "Sentence Average LD: 0.7591 \n",
            "Comment Average LD: 0.9441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_metrics_individual(comments_corpus, 'houseboat_tt.txt', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbKw6WvUwKUU",
        "outputId": "c4f72944-5c72-4bf4-84fd-b15b3e6b7d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======TEXT INFO========\n",
            "syllable done\n",
            "this text has 244 words, 27 sentences, and 305 syllables, and 20 comments.\n",
            "\n",
            "=======SENTIMENT FINDER========\n",
            "<class 'list'>\n",
            "Total Polarity Score: 0.1969 \n",
            "Average Polarity Score per Sentence: -0.0016\n",
            "\n",
            "=======FLESCH READING EASE========\n",
            "Overall Score: 91.912, Average Score per Comment: 85.087\n",
            "\n",
            "=======AGE OF ACQUISITION========\n",
            "Whole Average Comment Section AOA: 4.8 \n",
            "Average AOA per Comment: 5.31\n",
            "\n",
            "=======NOUN DEPENDENCIES========\n",
            "Average Noun Dependancy: 1.2321 \n",
            "Standard Deviation: 0.934\n",
            "\n",
            "=======FIND LD========\n",
            "Overall LD: 0.0228 \n",
            "Sentence Average LD: 0.8929 \n",
            "Comment Average LD: 0.9617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The comment count is higher on TT despite it having less words. This time it seems the TT comment section has a less positive sentiment compared to IG. It is a rather significant difference. The readability is much higher on TT than it is to IG. This time, the AoA is higher for TT comments than it usually is, overpassing IG comments. The TT comments also have a higher dependency but the IG comments have a slightly higher standard deviation. Although the overall and comment average have similar LD values, the sentence average of the TT comments is notably higher."
      ],
      "metadata": {
        "id": "dPacj6OtKqKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MOM"
      ],
      "metadata": {
        "id": "cMpMEycZ3nV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_metrics_individual(comments_corpus, 'mom_ig.txt', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfFGQWD8wV-C",
        "outputId": "dfc911bb-29ce-4780-d0e1-315b3abf058e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======TEXT INFO========\n",
            "syllable done\n",
            "this text has 368 words, 35 sentences, and 472 syllables, and 17 comments.\n",
            "\n",
            "=======SENTIMENT FINDER========\n",
            "<class 'list'>\n",
            "Total Polarity Score: 0.9611 \n",
            "Average Polarity Score per Sentence: 0.0726\n",
            "\n",
            "=======FLESCH READING EASE========\n",
            "Overall Score: 87.654, Average Score per Comment: 84.654\n",
            "\n",
            "=======AGE OF ACQUISITION========\n",
            "Whole Average Comment Section AOA: 4.8 \n",
            "Average AOA per Comment: 4.71\n",
            "\n",
            "=======NOUN DEPENDENCIES========\n",
            "Average Noun Dependancy: 1.6111 \n",
            "Standard Deviation: 1.1229\n",
            "\n",
            "=======FIND LD========\n",
            "Overall LD: 0.017 \n",
            "Sentence Average LD: 0.8044 \n",
            "Comment Average LD: 0.9269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_metrics_individual(comments_corpus, 'mom_tt.txt', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Eb5aiHzwYEl",
        "outputId": "b0a9ce11-1983-4af5-ae9a-e0471caa40a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======TEXT INFO========\n",
            "syllable done\n",
            "this text has 298 words, 49 sentences, and 364 syllables, and 45 comments.\n",
            "\n",
            "=======SENTIMENT FINDER========\n",
            "<class 'list'>\n",
            "Total Polarity Score: 0.9991 \n",
            "Average Polarity Score per Sentence: 0.3199\n",
            "\n",
            "=======FLESCH READING EASE========\n",
            "Overall Score: 97.325, Average Score per Comment: 91.383\n",
            "\n",
            "=======AGE OF ACQUISITION========\n",
            "Whole Average Comment Section AOA: 4.4 \n",
            "Average AOA per Comment: 4.49\n",
            "\n",
            "=======NOUN DEPENDENCIES========\n",
            "Average Noun Dependancy: 1.8478 \n",
            "Standard Deviation: 1.4601\n",
            "\n",
            "=======FIND LD========\n",
            "Overall LD: 0.0226 \n",
            "Sentence Average LD: 0.943 \n",
            "Comment Average LD: 0.9616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The comments on IG is significantly higher despite it having much less words than the TT comment section. This may have been an error on my part to choose this video. The general polarity score for both the IG and TT comment sections are quite similar and positive, but the avg per sentence has a big difference with the TT comments having a higher score. The readability, as always, is higher on TT's' comment section. The AoA is higher on IG comments but this time they are quite similar since both are around the age of 4. The noun dependancy is higher on the TT commets with also a higher standard deviation. The lexical diversity is higher in the TT comment section on all aspects, with a somewhat significant difference in the sentence average per LD."
      ],
      "metadata": {
        "id": "SsKNpTUsMdEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIEW"
      ],
      "metadata": {
        "id": "5Z1MUxlD3o5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_metrics_individual(comments_corpus, 'view_ig.txt', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7arkGvf05qa",
        "outputId": "bb52092b-af64-4814-ebf5-9f828ae5798f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======TEXT INFO========\n",
            "syllable done\n",
            "this text has 302 words, 31 sentences, and 421 syllables, and 25 comments.\n",
            "\n",
            "=======SENTIMENT FINDER========\n",
            "<class 'list'>\n",
            "Total Polarity Score: 0.9972 \n",
            "Average Polarity Score per Sentence: 0.2612\n",
            "\n",
            "=======FLESCH READING EASE========\n",
            "Overall Score: 79.011, Average Score per Comment: 74.906\n",
            "\n",
            "=======AGE OF ACQUISITION========\n",
            "Whole Average Comment Section AOA: 4.9 \n",
            "Average AOA per Comment: 4.9\n",
            "\n",
            "=======NOUN DEPENDENCIES========\n",
            "Average Noun Dependancy: 1.619 \n",
            "Standard Deviation: 1.1972\n",
            "\n",
            "=======FIND LD========\n",
            "Overall LD: 0.0176 \n",
            "Sentence Average LD: 0.9073 \n",
            "Comment Average LD: 0.9601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_metrics_individual(comments_corpus, 'view_tt.txt', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b50NKiHw058n",
        "outputId": "59279b71-f027-4791-cf52-709d974bdd80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======TEXT INFO========\n",
            "syllable done\n",
            "this text has 312 words, 34 sentences, and 407 syllables, and 26 comments.\n",
            "\n",
            "=======SENTIMENT FINDER========\n",
            "<class 'list'>\n",
            "Total Polarity Score: 0.9943 \n",
            "Average Polarity Score per Sentence: 0.1254\n",
            "\n",
            "=======FLESCH READING EASE========\n",
            "Overall Score: 87.161, Average Score per Comment: 86.752\n",
            "\n",
            "=======AGE OF ACQUISITION========\n",
            "Whole Average Comment Section AOA: 5.0 \n",
            "Average AOA per Comment: 5.09\n",
            "\n",
            "=======NOUN DEPENDENCIES========\n",
            "Average Noun Dependancy: 1.383 \n",
            "Standard Deviation: 1.0332\n",
            "\n",
            "=======FIND LD========\n",
            "Overall LD: 0.0174 \n",
            "Sentence Average LD: 0.8643 \n",
            "Comment Average LD: 0.9661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is only 1 comment difference between the two, and they both have similar amount of words. The overall polarity score of both comment sections have matching values, but the polarity score per sentence is higher for the IG comments. As always, the readability for TT comments are higher than IG's comment section. This time, the AoA is higher in the TT comment section, with it being age 5, as compared to IG's comment section being just under 5. The noun dependency is higher with IG this time, along with the standard deviation. The lexical diversity values on both comment sections are quite similar except for the sentence average, to which the IG comments got it higher."
      ],
      "metadata": {
        "id": "FpkZeSvAQVLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RESULTS AND REPORT"
      ],
      "metadata": {
        "id": "hZKhck3G50n7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overall Results\n",
        "\n",
        "These are the results how many times each comment section had the higher results for each metric:\n",
        "\n",
        "Comment Count\n",
        "\n",
        "IG: 0     \n",
        "TT: 7\n",
        "\n",
        "Polarity score\n",
        "\n",
        "IG: 3            \n",
        "TT: 4\n",
        "\n",
        "Readability\n",
        "\n",
        "IG: 1.5    \n",
        "TT: 5.5\n",
        "\n",
        "AoA\n",
        "\n",
        "IG: 5    \n",
        "TT: 2\n",
        "\n",
        "Noun Dependancy (and sd)\n",
        "\n",
        "IG: 3 (4)  \n",
        "TT: 4 (3)\n",
        "\n",
        "LD (Generally)\n",
        "\n",
        "IG: 1   \n",
        "TT: 6"
      ],
      "metadata": {
        "id": "DG4SoDOF46mE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c0UwQl3BkMcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Judging from the calculations of the program. Just from the comment count we can gather that tiktok tends to have shorter comments, while IG has possibility for wordier and longer comments. The polarity score count tells us that the difference isn't stark as I thought it would be, but it still indicate that Tiktok would be have a higher chance at being a more positive comment section. The readability count shows that tiktok tends to have a more easily digestible comment section that instagram, which could paired with the Age of Acquisition for tiktok generally being lower than Instagram. Although the noun dependency seems to be on even ground for the most part, the Tiktok comment section would likely be higher. Which is rather odd considering the other statistics implying\n",
        "a more simplicistic nature when it comes to tiktok. However, the instagram comments tend to be more consistent, probably implying that the tend to repeat words more. This matches up with the lexical diversity results, with tiktok having a higher count implying that there is less of a repeating thing.\n",
        "\n",
        "The differences in the polarity scores aren't usually that contrasting, so in regards to my first research question, it appears that there isn't really a big difference when it comes to emotions, but it is very situational. For example, in the drummer contemporary art video the difference between the two platforms were big, but that was one out of the seven videos brought up. Although I do hear many remarks on the internet about how Instagram has ruthless, blunt comments. Perhaps the videos that seem to have the negative sentiment could be influenced by a prefixed standard, as in the people who comment on instagram tend to be meaner because everyone implies that they are, and they want to fulfill that expectation for reasons such as humour. That, or the VADER sentiment feature could be working incorrectly.\n",
        "\n",
        "In regards to my second question, I can say that the intricacy depends on whether you are referring to the entire comment section or each comment, and it also depends on the kind of video. From the results, generally it feels like Tiktok comments would have more variety in their comments. With the fact that the lexical diversity and the noun dependency being generally higher (with the addition of the standard deviation being a lower count), this implies that tiktok is more complex in terms of the entire comment section, and that each comment is likely to be different from one another. Instagram on the other hand is intricate in a very niche sense. From the results, Instagram tends to have longer comments than on Tiktok. It also has less of a readability score and the age of acquisition tends to be higher (but not significantly big difference). This probably suggests how the individual comments of Instagram are perhaps more meticulous than the individual ones on Tiktok. Although you can see that that although Instagram has a lower noun dependency, it generally has a somewhat higher standard deviation. This implies consistency within the comment section. I did mention earlier about expectations and people on the internet wanting to fulfill them being a contributer to the nature of the comments. As I was browsing through the comment sections, I did notice that despite that there was this trend of repeating phrases, there are comments that are completely a different vibe compared to most. These comments take shape in  relatively sizeable paragraphs typed in a formal matter. These are not in every Instagram comment section, but they can be occasionally seen. This lead me to conclude that instagram comments are intricate in a sense that there are specific comments that offer a good sense of being meticulous, but not in the sense that generalizes the whole comment section.\n",
        "\n",
        "The video that highlighted the mean, aggressive reputation of instagram comments was the dog video, I included it to see the differences in polarity score but to my surprise the contrast wasn't as stark. In fact, the instagram comments was shown to have a higher polarity score in the specific video. However, thinking about it now, it is probably due to the fact that the true intent of comments can't really be captured by a computer without being carefully handpicked. The instagram comments on the dog video probably had words that had high sentiment, but used in a sarcastic and threatening way. I suppose this was one of the flaws that's present in the project.\n",
        "\n",
        "Overall, the project was done rather roughly. One thing that prevented me from being able to work on the project smoothly was the fact that I went out on a holiday and the fatigue was getting over me. I think Python being a language that I'm not accustomed to has also served as an issue for me. I haven't been able to meddle and tweak the program as much as I'd like to since I figured it would take awhile trying to figure out syntax, and I was already on a time crunch from being on holiday. As a result, I just copied and pasted from my previous assignment and added the new features that I thought would fit for the project and made slight changes according to fit the kind of data that I used. Because of this, I think my program could definitely be improved, and I felt like there could be more metrics that I could touch upon and link it had I got the time. I would've also made a function that would gather up the results of both of the comment sections that I am comparing under a specific video.\n",
        "\n",
        "As for the data, I'm not sure if it was enough since I wasn't used to working with such a small group of texts. It was originally supposed to be 5 videos but I decided to include 2 more for data's sake. I feel like more videos would've been good so that we could accurately measure the differences more using the counts. I also tried my best to try include all sides of the internet into 7 vidoes, I did have some difficulty trying to find the right videos since I had to check if the comments fit my criteria. At worst there would be a video that I would find interesting to analyze on but to my luck there would be little to no comments. The data gathering could definitely be improved. Again, I suppose the timing of my holiday likely affected this.\n"
      ],
      "metadata": {
        "id": "DJ64-GPSb5ny"
      }
    }
  ]
}